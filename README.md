# RNN Implementation from Scratch

A complete implementation of a Recurrent Neural Network (RNN) built entirely from scratch using NumPy. This project demonstrates the fundamental concepts of RNNs, including forward propagation, backpropagation through time (BPTT), and training on sequence data.

## Features

- **SimpleRNN Class**: A basic RNN implementation with configurable input, hidden, and output sizes
- **Activation Functions**: Custom implementations of tanh, softmax, and cross-entropy loss
- **Training Loop**: Complete training function with gradient descent optimization
- **Example Usage**: Demonstration with randomly generated sequence data for binary classification
